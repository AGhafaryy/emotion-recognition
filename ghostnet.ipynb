{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1eae77e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: timm in /home/tdandawate/.local/lib/python3.9/site-packages (0.9.2)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.9/site-packages (from timm) (0.10.0+cu111)\n",
      "Requirement already satisfied: safetensors in /home/tdandawate/.local/lib/python3.9/site-packages (from timm) (0.3.1)\n",
      "Requirement already satisfied: torch>=1.7 in /opt/conda/lib/python3.9/site-packages (from timm) (1.9.0+cu111)\n",
      "Requirement already satisfied: huggingface-hub in /home/tdandawate/.local/lib/python3.9/site-packages (from timm) (0.14.1)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.9/site-packages (from timm) (5.4.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch>=1.7->timm) (4.5.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from huggingface-hub->timm) (2.26.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub->timm) (21.0)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.9/site-packages (from huggingface-hub->timm) (2021.7.0)\n",
      "Requirement already satisfied: filelock in /home/tdandawate/.local/lib/python3.9/site-packages (from huggingface-hub->timm) (3.12.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub->timm) (4.61.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging>=20.9->huggingface-hub->timm) (2.4.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->huggingface-hub->timm) (2.0.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->huggingface-hub->timm) (3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->huggingface-hub->timm) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->huggingface-hub->timm) (2022.12.7)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from torchvision->timm) (1.22.4)\n",
      "Requirement already satisfied: pillow>=5.3.0 in /opt/conda/lib/python3.9/site-packages (from torchvision->timm) (8.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44d9bbf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found device: GeForce GTX 1080 Ti, n_gpu: 1\n",
      "Cuda version: 11.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Confirm that the GPU is detected\n",
    "\n",
    "assert torch.cuda.is_available()\n",
    "\n",
    "# Get the GPU device name.\n",
    "device_name = torch.cuda.get_device_name()\n",
    "n_gpu = torch.cuda.device_count()\n",
    "print(f\"Found device: {device_name}, n_gpu: {n_gpu}\")\n",
    "print(f'Cuda version: {torch.version.cuda}')\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c5e8a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -upy-cuda11x (/home/tdandawate/.local/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting cupy-cuda11x\n",
      "  Using cached cupy_cuda11x-12.1.0-cp39-cp39-manylinux2014_x86_64.whl (89.9 MB)\n",
      "Requirement already satisfied: numpy<1.27,>=1.20 in /opt/conda/lib/python3.9/site-packages (from cupy-cuda11x) (1.22.4)\n",
      "Requirement already satisfied: fastrlock>=0.5 in /home/tdandawate/.local/lib/python3.9/site-packages (from cupy-cuda11x) (0.8.1)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -upy-cuda11x (/home/tdandawate/.local/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: cupy-cuda11x\n",
      "Successfully installed cupy-cuda11x-12.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install cupy-cuda11x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20d70be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /tmp/xdg-cache/torch/hub/huawei-noah_ghostnet_master\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\n================================================================\nFailed to import CuPy.\n\nIf you installed CuPy via wheels (cupy-cudaXXX or cupy-rocm-X-X), make sure that the package matches with the version of CUDA or ROCm installed.\n\nOn Linux, you may need to set LD_LIBRARY_PATH environment variable depending on how you installed CUDA/ROCm.\nOn Windows, try setting CUDA_PATH environment variable.\n\nCheck the Installation Guide for details:\n  https://docs.cupy.dev/en/latest/install.html\n\nOriginal error:\n  ImportError: /home/tdandawate/.local/lib/python3.9/site-packages/cupy_backends/cuda/api/runtime.cpython-39-x86_64-linux-gnu.so: undefined symbol: cudaMemPoolCreate, version libcudart.so.11.0\n================================================================\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/cupy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mcupy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_core\u001b[0m  \u001b[0;31m# NOQA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/cupy/_core/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcupy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_core\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcore\u001b[0m  \u001b[0;31m# NOQA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcupy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_core\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfusion\u001b[0m  \u001b[0;31m# NOQA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mcupy/_core/core.pyx\u001b[0m in \u001b[0;36minit cupy._core.core\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/cupy/cuda/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcupy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_environment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_hipcc_path\u001b[0m  \u001b[0;31m# NOQA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcupy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompiler\u001b[0m  \u001b[0;31m# NOQA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcupy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdevice\u001b[0m  \u001b[0;31m# NOQA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/cupy/cuda/compiler.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcupy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcupy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mcupy/cuda/device.pyx\u001b[0m in \u001b[0;36minit cupy.cuda.device\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: /home/tdandawate/.local/lib/python3.9/site-packages/cupy_backends/cuda/api/runtime.cpython-39-x86_64-linux-gnu.so: undefined symbol: cudaMemPoolCreate, version libcudart.so.11.0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1549/3384396781.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtimm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'huawei-noah/ghostnet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ghostnet_1x'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/hub.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(repo_or_dir, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0mrepo_or_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_cache_or_reload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_or_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_reload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_or_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/hub.py\u001b[0m in \u001b[0;36m_load_local\u001b[0;34m(hubconf_dir, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m     \u001b[0mhubconf_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhubconf_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMODULE_HUBCONF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m     \u001b[0mhub_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODULE_HUBCONF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhubconf_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[0mentry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_entry_from_hubconf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhub_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/hub.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, path)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule_from_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLoader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexec_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "\u001b[0;32m/tmp/xdg-cache/torch/hub/huawei-noah_ghostnet_master/hubconf.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mghostnet_pytorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mghostnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mghostnet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msnnmlp_pytorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msnn_mlp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSNNMLP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/xdg-cache/torch/hub/huawei-noah_ghostnet_master/snnmlp_pytorch/models/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 2022.06.27-Changed for building SNN-MLP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#            Huawei Technologies Co., Ltd. <foss@huawei.com>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msnn_mlp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSNNMLP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/xdg-cache/torch/hub/huawei-noah_ghostnet_master/snnmlp_pytorch/models/snn_mlp.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtimm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDropPath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_2tuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrunc_normal_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msnn_cuda\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLIFSpike\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mMlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/xdg-cache/torch/hub/huawei-noah_ghostnet_master/snnmlp_pytorch/models/snn_cuda.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnamedtuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcupy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstring\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTemplate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/cupy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mcupy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_core\u001b[0m  \u001b[0;31m# NOQA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     raise ImportError(f'''\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0m_environment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_diagnose_import_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: \n================================================================\nFailed to import CuPy.\n\nIf you installed CuPy via wheels (cupy-cudaXXX or cupy-rocm-X-X), make sure that the package matches with the version of CUDA or ROCm installed.\n\nOn Linux, you may need to set LD_LIBRARY_PATH environment variable depending on how you installed CUDA/ROCm.\nOn Windows, try setting CUDA_PATH environment variable.\n\nCheck the Installation Guide for details:\n  https://docs.cupy.dev/en/latest/install.html\n\nOriginal error:\n  ImportError: /home/tdandawate/.local/lib/python3.9/site-packages/cupy_backends/cuda/api/runtime.cpython-39-x86_64-linux-gnu.so: undefined symbol: cudaMemPoolCreate, version libcudart.so.11.0\n================================================================\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "\n",
    "model = torch.hub.load('huawei-noah/ghostnet', 'ghostnet_1x', pretrained=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f807021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -upy-cuda11x (/home/tdandawate/.local/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: paddlehub in /home/tdandawate/.local/lib/python3.9/site-packages (2.3.1)\n",
      "Requirement already satisfied: colorama in /opt/conda/lib/python3.9/site-packages (from paddlehub) (0.4.6)\n",
      "Requirement already satisfied: colorlog in /home/tdandawate/.local/lib/python3.9/site-packages (from paddlehub) (6.7.0)\n",
      "Requirement already satisfied: easydict in /home/tdandawate/.local/lib/python3.9/site-packages (from paddlehub) (1.10)\n",
      "Requirement already satisfied: filelock in /home/tdandawate/.local/lib/python3.9/site-packages (from paddlehub) (3.12.0)\n",
      "Requirement already satisfied: flask>=1.1.0 in /home/tdandawate/.local/lib/python3.9/site-packages (from paddlehub) (2.3.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from paddlehub) (1.22.4)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.9/site-packages (from paddlehub) (3.4.2)\n",
      "Requirement already satisfied: opencv-python in /opt/conda/lib/python3.9/site-packages (from paddlehub) (4.7.0.72)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from paddlehub) (21.0)\n",
      "Requirement already satisfied: paddle2onnx>=0.5.1 in /home/tdandawate/.local/lib/python3.9/site-packages (from paddlehub) (1.0.6)\n",
      "Requirement already satisfied: paddlenlp>=2.0.0 in /home/tdandawate/.local/lib/python3.9/site-packages (from paddlehub) (2.5.2)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.9/site-packages (from paddlehub) (8.3.1)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.9/site-packages (from paddlehub) (5.4.1)\n",
      "Requirement already satisfied: pyzmq in /opt/conda/lib/python3.9/site-packages (from paddlehub) (25.0.2)\n",
      "Requirement already satisfied: rarfile in /home/tdandawate/.local/lib/python3.9/site-packages (from paddlehub) (4.0)\n",
      "Requirement already satisfied: tqdm in /home/tdandawate/.local/lib/python3.9/site-packages (from paddlehub) (4.65.0)\n",
      "Requirement already satisfied: visualdl>=2.0.0 in /home/tdandawate/.local/lib/python3.9/site-packages (from paddlehub) (2.4.2)\n",
      "Requirement already satisfied: gradio in /home/tdandawate/.local/lib/python3.9/site-packages (from paddlehub) (3.32.0)\n",
      "Requirement already satisfied: gunicorn>=19.10.0 in /home/tdandawate/.local/lib/python3.9/site-packages (from paddlehub) (20.1.0)\n",
      "Requirement already satisfied: Werkzeug>=2.3.3 in /home/tdandawate/.local/lib/python3.9/site-packages (from flask>=1.1.0->paddlehub) (2.3.4)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in /home/tdandawate/.local/lib/python3.9/site-packages (from flask>=1.1.0->paddlehub) (3.1.2)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in /home/tdandawate/.local/lib/python3.9/site-packages (from flask>=1.1.0->paddlehub) (2.1.2)\n",
      "Requirement already satisfied: click>=8.1.3 in /home/tdandawate/.local/lib/python3.9/site-packages (from flask>=1.1.0->paddlehub) (8.1.3)\n",
      "Requirement already satisfied: blinker>=1.6.2 in /home/tdandawate/.local/lib/python3.9/site-packages (from flask>=1.1.0->paddlehub) (1.6.2)\n",
      "Requirement already satisfied: importlib-metadata>=3.6.0 in /opt/conda/lib/python3.9/site-packages (from flask>=1.1.0->paddlehub) (6.1.0)\n",
      "Requirement already satisfied: setuptools>=3.0 in /home/tdandawate/.local/lib/python3.9/site-packages (from gunicorn>=19.10.0->paddlehub) (67.8.0)\n",
      "Requirement already satisfied: jieba in /home/tdandawate/.local/lib/python3.9/site-packages (from paddlenlp>=2.0.0->paddlehub) (0.42.1)\n",
      "Requirement already satisfied: seqeval in /home/tdandawate/.local/lib/python3.9/site-packages (from paddlenlp>=2.0.0->paddlehub) (1.2.2)\n",
      "Requirement already satisfied: dill<0.3.5 in /opt/conda/lib/python3.9/site-packages (from paddlenlp>=2.0.0->paddlehub) (0.3.4)\n",
      "Requirement already satisfied: multiprocess<=0.70.12.2 in /home/tdandawate/.local/lib/python3.9/site-packages (from paddlenlp>=2.0.0->paddlehub) (0.70.12.2)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /home/tdandawate/.local/lib/python3.9/site-packages (from paddlenlp>=2.0.0->paddlehub) (2.12.0)\n",
      "Requirement already satisfied: paddlefsl in /home/tdandawate/.local/lib/python3.9/site-packages (from paddlenlp>=2.0.0->paddlehub) (1.1.0)\n",
      "Requirement already satisfied: sentencepiece in /home/tdandawate/.local/lib/python3.9/site-packages (from paddlenlp>=2.0.0->paddlehub) (0.1.99)\n",
      "Requirement already satisfied: huggingface-hub>=0.11.1 in /home/tdandawate/.local/lib/python3.9/site-packages (from paddlenlp>=2.0.0->paddlehub) (0.14.1)\n",
      "Requirement already satisfied: Flask-Babel<3.0.0 in /home/tdandawate/.local/lib/python3.9/site-packages (from paddlenlp>=2.0.0->paddlehub) (2.0.0)\n",
      "Requirement already satisfied: fastapi in /home/tdandawate/.local/lib/python3.9/site-packages (from paddlenlp>=2.0.0->paddlehub) (0.95.2)\n",
      "Requirement already satisfied: uvicorn in /home/tdandawate/.local/lib/python3.9/site-packages (from paddlenlp>=2.0.0->paddlehub) (0.22.0)\n",
      "Requirement already satisfied: typer in /home/tdandawate/.local/lib/python3.9/site-packages (from paddlenlp>=2.0.0->paddlehub) (0.9.0)\n",
      "Requirement already satisfied: rich in /home/tdandawate/.local/lib/python3.9/site-packages (from paddlenlp>=2.0.0->paddlehub) (13.3.5)\n",
      "Requirement already satisfied: bce-python-sdk in /home/tdandawate/.local/lib/python3.9/site-packages (from visualdl>=2.0.0->paddlehub) (0.8.83)\n",
      "Requirement already satisfied: protobuf>=3.11.0 in /opt/conda/lib/python3.9/site-packages (from visualdl>=2.0.0->paddlehub) (3.20.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from visualdl>=2.0.0->paddlehub) (2.26.0)\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.9/site-packages (from visualdl>=2.0.0->paddlehub) (1.16.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from visualdl>=2.0.0->paddlehub) (1.5.3)\n",
      "Requirement already satisfied: aiofiles in /home/tdandawate/.local/lib/python3.9/site-packages (from gradio->paddlehub) (23.1.0)\n",
      "Requirement already satisfied: aiohttp in /home/tdandawate/.local/lib/python3.9/site-packages (from gradio->paddlehub) (3.8.4)\n",
      "Requirement already satisfied: altair>=4.2.0 in /home/tdandawate/.local/lib/python3.9/site-packages (from gradio->paddlehub) (5.0.1)\n",
      "Requirement already satisfied: ffmpy in /home/tdandawate/.local/lib/python3.9/site-packages (from gradio->paddlehub) (0.3.0)\n",
      "Requirement already satisfied: gradio-client>=0.2.4 in /home/tdandawate/.local/lib/python3.9/site-packages (from gradio->paddlehub) (0.2.5)\n",
      "Requirement already satisfied: httpx in /home/tdandawate/.local/lib/python3.9/site-packages (from gradio->paddlehub) (0.24.1)\n",
      "Requirement already satisfied: markdown-it-py[linkify]>=2.0.0 in /home/tdandawate/.local/lib/python3.9/site-packages (from gradio->paddlehub) (2.2.0)\n",
      "Requirement already satisfied: markupsafe in /opt/conda/lib/python3.9/site-packages (from gradio->paddlehub) (2.1.2)\n",
      "Requirement already satisfied: mdit-py-plugins<=0.3.3 in /home/tdandawate/.local/lib/python3.9/site-packages (from gradio->paddlehub) (0.3.3)\n",
      "Requirement already satisfied: orjson in /home/tdandawate/.local/lib/python3.9/site-packages (from gradio->paddlehub) (3.8.14)\n",
      "Requirement already satisfied: pydantic in /home/tdandawate/.local/lib/python3.9/site-packages (from gradio->paddlehub) (1.10.8)\n",
      "Requirement already satisfied: pydub in /home/tdandawate/.local/lib/python3.9/site-packages (from gradio->paddlehub) (0.25.1)\n",
      "Requirement already satisfied: pygments>=2.12.0 in /home/tdandawate/.local/lib/python3.9/site-packages (from gradio->paddlehub) (2.15.1)\n",
      "Requirement already satisfied: python-multipart in /home/tdandawate/.local/lib/python3.9/site-packages (from gradio->paddlehub) (0.0.6)\n",
      "Requirement already satisfied: semantic-version in /home/tdandawate/.local/lib/python3.9/site-packages (from gradio->paddlehub) (2.10.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from gradio->paddlehub) (4.5.0)\n",
      "Requirement already satisfied: websockets>=10.0 in /home/tdandawate/.local/lib/python3.9/site-packages (from gradio->paddlehub) (11.0.3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.9/site-packages (from matplotlib->paddlehub) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib->paddlehub) (1.3.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib->paddlehub) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.9/site-packages (from matplotlib->paddlehub) (2.8.2)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.9/site-packages (from altair>=4.2.0->gradio->paddlehub) (3.2.0)\n",
      "Requirement already satisfied: toolz in /opt/conda/lib/python3.9/site-packages (from altair>=4.2.0->gradio->paddlehub) (0.11.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/tdandawate/.local/lib/python3.9/site-packages (from datasets>=2.0.0->paddlenlp>=2.0.0->paddlehub) (12.0.0)\n",
      "Requirement already satisfied: xxhash in /home/tdandawate/.local/lib/python3.9/site-packages (from datasets>=2.0.0->paddlenlp>=2.0.0->paddlehub) (3.2.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/tdandawate/.local/lib/python3.9/site-packages (from datasets>=2.0.0->paddlenlp>=2.0.0->paddlehub) (2023.5.0)\n",
      "Requirement already satisfied: responses<0.19 in /home/tdandawate/.local/lib/python3.9/site-packages (from datasets>=2.0.0->paddlenlp>=2.0.0->paddlehub) (0.18.0)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.9/site-packages (from Flask-Babel<3.0.0->paddlenlp>=2.0.0->paddlehub) (2021.1)\n",
      "Requirement already satisfied: Babel>=2.3 in /opt/conda/lib/python3.9/site-packages (from Flask-Babel<3.0.0->paddlenlp>=2.0.0->paddlehub) (2.9.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.9/site-packages (from importlib-metadata>=3.6.0->flask>=1.1.0->paddlehub) (3.5.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/tdandawate/.local/lib/python3.9/site-packages (from markdown-it-py[linkify]>=2.0.0->gradio->paddlehub) (0.1.2)\n",
      "Requirement already satisfied: linkify-it-py<3,>=1 in /home/tdandawate/.local/lib/python3.9/site-packages (from markdown-it-py[linkify]>=2.0.0->gradio->paddlehub) (2.0.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->visualdl>=2.0.0->paddlehub) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->visualdl>=2.0.0->paddlehub) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->visualdl>=2.0.0->paddlehub) (2.0.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->visualdl>=2.0.0->paddlehub) (3.1)\n",
      "Requirement already satisfied: h11>=0.8 in /home/tdandawate/.local/lib/python3.9/site-packages (from uvicorn->paddlenlp>=2.0.0->paddlehub) (0.14.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->gradio->paddlehub) (21.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/tdandawate/.local/lib/python3.9/site-packages (from aiohttp->gradio->paddlehub) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/tdandawate/.local/lib/python3.9/site-packages (from aiohttp->gradio->paddlehub) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/tdandawate/.local/lib/python3.9/site-packages (from aiohttp->gradio->paddlehub) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/tdandawate/.local/lib/python3.9/site-packages (from aiohttp->gradio->paddlehub) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/tdandawate/.local/lib/python3.9/site-packages (from aiohttp->gradio->paddlehub) (1.3.1)\n",
      "Requirement already satisfied: pycryptodome>=3.8.0 in /home/tdandawate/.local/lib/python3.9/site-packages (from bce-python-sdk->visualdl>=2.0.0->paddlehub) (3.18.0)\n",
      "Requirement already satisfied: future>=0.6.0 in /home/tdandawate/.local/lib/python3.9/site-packages (from bce-python-sdk->visualdl>=2.0.0->paddlehub) (0.18.3)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /home/tdandawate/.local/lib/python3.9/site-packages (from fastapi->paddlenlp>=2.0.0->paddlehub) (0.27.0)\n",
      "Requirement already satisfied: httpcore<0.18.0,>=0.15.0 in /home/tdandawate/.local/lib/python3.9/site-packages (from httpx->gradio->paddlehub) (0.17.2)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.9/site-packages (from httpx->gradio->paddlehub) (1.2.0)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.9/site-packages (from seqeval->paddlenlp>=2.0.0->paddlehub) (0.24.2)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /home/tdandawate/.local/lib/python3.9/site-packages (from httpcore<0.18.0,>=0.15.0->httpx->gradio->paddlehub) (3.6.2)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.9/site-packages (from jsonschema>=3.0->altair>=4.2.0->gradio->paddlehub) (0.17.3)\n",
      "Requirement already satisfied: uc-micro-py in /home/tdandawate/.local/lib/python3.9/site-packages (from linkify-it-py<3,>=1->markdown-it-py[linkify]>=2.0.0->gradio->paddlehub) (1.0.2)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.9/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp>=2.0.0->paddlehub) (1.7.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.9/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp>=2.0.0->paddlehub) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp>=2.0.0->paddlehub) (2.2.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -upy-cuda11x (/home/tdandawate/.local/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mDefaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -upy-cuda11x (/home/tdandawate/.local/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting paddlepaddle\n",
      "  Downloading paddlepaddle-2.4.2-cp39-cp39-manylinux1_x86_64.whl (121.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.7/121.7 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.20.0 in /opt/conda/lib/python3.9/site-packages (from paddlepaddle) (2.26.0)\n",
      "Requirement already satisfied: numpy>=1.13 in /opt/conda/lib/python3.9/site-packages (from paddlepaddle) (1.22.4)\n",
      "Collecting protobuf<=3.20.0,>=3.1.0 (from paddlepaddle)\n",
      "  Downloading protobuf-3.20.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: Pillow in /opt/conda/lib/python3.9/site-packages (from paddlepaddle) (8.3.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.9/site-packages (from paddlepaddle) (1.16.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.9/site-packages (from paddlepaddle) (5.0.9)\n",
      "Collecting astor (from paddlepaddle)\n",
      "  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Collecting paddle-bfloat==0.1.7 (from paddlepaddle)\n",
      "  Downloading paddle_bfloat-0.1.7-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (383 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.1/383.1 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: opt-einsum==3.3.0 in /opt/conda/lib/python3.9/site-packages (from paddlepaddle) (3.3.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.20.0->paddlepaddle) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.20.0->paddlepaddle) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests>=2.20.0->paddlepaddle) (2.0.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.20.0->paddlepaddle) (3.1)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -upy-cuda11x (/home/tdandawate/.local/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0mInstalling collected packages: paddle-bfloat, protobuf, astor, paddlepaddle\n",
      "\u001b[33m  WARNING: The script fleetrun is installed in '/home/tdandawate/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.12.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.20.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed astor-0.8.1 paddle-bfloat-0.1.7 paddlepaddle-2.4.2 protobuf-3.20.0\n"
     ]
    }
   ],
   "source": [
    "!pip install paddlehub\n",
    "!pip install paddlepaddle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a0d1a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tdandawate/.local/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download https://bj.bcebos.com/paddlehub/paddlehub_dev/ghostnet_x1_3_imagenet.tar.gz\n",
      "[##################################################] 100.00%\n",
      "Decompress /home/tdandawate/.paddlehub/tmp/tmpzlzaixmb/ghostnet_x1_3_imagenet.tar.gz\n",
      "[##################################################] 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-05-26 19:22:37,495] [    INFO]\u001b[0m - Successfully installed ghostnet_x1_3_imagenet-1.0.0\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load pretrained checkpoint success\n"
     ]
    }
   ],
   "source": [
    "import paddlehub as hub\n",
    "\n",
    "model = hub.Module(name='ghostnet_x1_3_imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3aad143f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GhostNet(\n",
       "  (conv1): ConvBNLayer(\n",
       "    (_conv): Conv2D(3, 20, kernel_size=[3, 3], stride=[2, 2], padding=1, data_format=NCHW)\n",
       "    (_batch_norm): BatchNorm()\n",
       "  )\n",
       "  (_ghostbottleneck_0): GhostBottleneck(\n",
       "    (ghost_module_1): GhostModule(\n",
       "      (primary_conv): ConvBNLayer(\n",
       "        (_conv): Conv2D(20, 10, kernel_size=[1, 1], data_format=NCHW)\n",
       "        (_batch_norm): BatchNorm()\n",
       "      )\n",
       "      (cheap_operation): ConvBNLayer(\n",
       "        (_conv): Conv2D(10, 10, kernel_size=[3, 3], padding=1, groups=10, data_format=NCHW)\n",
       "        (_batch_norm): BatchNorm()\n",
       "      )\n",
       "    )\n",
       "    (ghost_module_2): GhostModule(\n",
       "      (primary_conv): ConvBNLayer(\n",
       "        (_conv): Conv2D(20, 10, kernel_size=[1, 1], data_format=NCHW)\n",
       "        (_batch_norm): BatchNorm()\n",
       "      )\n",
       "      (cheap_operation): ConvBNLayer(\n",
       "        (_conv): Conv2D(10, 10, kernel_size=[3, 3], padding=1, groups=10, data_format=NCHW)\n",
       "        (_batch_norm): BatchNorm()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (_ghostbottleneck_1): GhostBottleneck(\n",
       "    (ghost_module_1): GhostModule(\n",
       "      (primary_conv): ConvBNLayer(\n",
       "        (_conv): Conv2D(20, 32, kernel_size=[1, 1], data_format=NCHW)\n",
       "        (_batch_norm): BatchNorm()\n",
       "      )\n",
       "      (cheap_operation): ConvBNLayer(\n",
       "        (_conv): Conv2D(32, 32, kernel_size=[3, 3], padding=1, groups=32, data_format=NCHW)\n",
       "        (_batch_norm): BatchNorm()\n",
       "      )\n",
       "    )\n",
       "    (depthwise_conv): ConvBNLayer(\n",
       "      (_conv): Conv2D(64, 64, kernel_size=[3, 3], stride=[2, 2], padding=1, groups=64, data_format=NCHW)\n",
       "      (_batch_norm): BatchNorm()\n",
       "    )\n",
       "    (ghost_module_2): GhostModule(\n",
       "      (primary_conv): ConvBNLayer(\n",
       "        (_conv): Conv2D(64, 16, kernel_size=[1, 1], data_format=NCHW)\n",
       "        (_batch_norm): BatchNorm()\n",
       "      )\n",
       "      (cheap_operation): ConvBNLayer(\n",
       "        (_conv): Conv2D(16, 16, kernel_size=[3, 3], padding=1, groups=16, data_format=NCHW)\n",
       "        (_batch_norm): BatchNorm()\n",
       "      )\n",
       "    )\n",
       "    (shortcut_depthwise): ConvBNLayer(\n",
       "      (_conv): Conv2D(20, 20, kernel_size=[3, 3], stride=[2, 2], padding=1, groups=20, data_format=NCHW)\n",
       "      (_batch_norm): BatchNorm()\n",
       "    )\n",
       "    (shortcut_conv): ConvBNLayer(\n",
       "      (_conv): Conv2D(20, 32, kernel_size=[1, 1], data_format=NCHW)\n",
       "      (_batch_norm): BatchNorm()\n",
       "    )\n",
       "  )\n",
       "  (_ghostbottleneck_2): GhostBottleneck(\n",
       "    (ghost_module_1): GhostModule(\n",
       "      (primary_conv): ConvBNLayer(\n",
       "        (_conv): Conv2D(32, 46, kernel_size=[1, 1], data_format=NCHW)\n",
       "        (_batch_norm): BatchNorm()\n",
       "      )\n",
       "      (cheap_operation): ConvBNLayer(\n",
       "        (_conv): Conv2D(46, 46, kernel_size=[3, 3], padding=1, groups=46, data_format=NCHW)\n",
       "        (_batch_norm): BatchNorm()\n",
       "      )\n",
       "    )\n",
       "    (ghost_module_2): GhostModule(\n",
       "      (primary_conv): ConvBNLayer(\n",
       "        (_conv): Conv2D(92, 16, kernel_size=[1, 1], data_format=NCHW)\n",
       "        (_batch_norm): BatchNorm()\n",
       "      )\n",
       "      (cheap_operation): ConvBNLayer(\n",
       "        (_conv): Conv2D(16, 16, kernel_size=[3, 3], padding=1, groups=16, data_format=NCHW)\n",
       "        (_batch_norm): BatchNorm()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (_ghostbottleneck_3): GhostBottleneck(\n",
       "    (ghost_module_1): GhostModule(\n",
       "      (primary_conv): ConvBNLayer(\n",
       "        (_conv): Conv2D(32, 46, kernel_size=[1, 1], data_format=NCHW)\n",
       "        (_batch_norm): BatchNorm()\n",
       "      )\n",
       "      (cheap_operation): ConvBNLayer(\n",
       "        (_conv): Conv2D(46, 46, kernel_size=[3, 3], padding=1, groups=46, data_format=NCHW)\n",
       "        (_batch_norm): BatchNorm()\n",
       "      )\n",
       "    )\n",
       "    (depthwise_conv): ConvBNLayer(\n",
       "      (_conv): Conv2D(92, 92, kernel_size=[5, 5], stride=[2, 2], padding=2, groups=92, data_format=NCHW)\n",
       "      (_batch_norm): BatchNorm()\n",
       "    )\n",
       "    (se_block): SEBlock(\n",
       "      (pool2d_gap): AdaptiveAvgPool2D(output_size=1)\n",
       "      (squeeze): Linear(in_features=92, out_features=23, dtype=float32)\n",
       "      (excitation): Linear(in_features=23, out_features=92, dtype=float32)\n",
       "    )\n",
       "    (ghost_module_2): GhostModule(\n",
       "      (primary_conv): ConvBNLayer(\n",
       "        (_conv): Conv2D(92, 26, kernel_size=[1, 1], data_format=NCHW)\n",
       "        (_batch_norm): BatchNorm()\n",
       "      )\n",
       "      (cheap_operation): ConvBNLayer(\n",
       "        (_conv): Conv2D(26, 26, kernel_size=[3, 3], padding=1, groups=26, data_format=NCHW)\n",
       "        (_batch_norm): BatchNorm()\n",
       "      )\n",
       "    )\n",
       "    (shortcut_depthwise): ConvBNLayer(\n",
       "      (_conv): Conv2D(32, 32, kernel_size=[5, 5], stride=[2, 2], padding=2, groups=32, data_format=NCHW)\n",
       "      (_batch_norm): BatchNorm()\n",
       "    )\n",
       "    (shortcut_conv): ConvBNLayer(\n",
       "      (_conv): Conv2D(32, 52, kernel_size=[1, 1], data_format=NCHW)\n",
       "      (_batch_norm): BatchNorm()\n",
       "    )\n",
       "  )\n",
       "  (_ghostbottleneck_4): GhostBottleneck(\n",
       "    (ghost_module_1): GhostModule(\n",
       "      (primary_conv): ConvBNLayer(\n",
       "        (_conv): Conv2D(52, 78, kernel_size=[1, 1], data_format=NCHW)\n",
       "        (_batch_norm): BatchNorm()\n",
       "      )\n",
       "      (cheap_operation): ConvBNLayer(\n",
       "        (_conv): Conv2D(78, 78, kernel_size=[3, 3], padding=1, groups=78, data_format=NCHW)\n",
       "        (_batch_norm): BatchNorm()\n",
       "      )\n",
       "    )\n",
       "    (se_block): SEBlock(\n",
       "      (pool2d_gap): AdaptiveAvgPool2D(output_size=1)\n",
       "      (squeeze): Linear(in_features=156, out_features=39, dtype=float32)\n",
       "      (excitation): Linear(in_features=39, out_features=156, dtype=float32)\n",
       "    )\n",
       "    (ghost_module_2): GhostModule(\n",
       "      (primary_conv): ConvBNLayer(\n",
       "        (_conv): Conv2D(156, 26, kernel_size=[1, 1], data_format=NCHW)\n",
       "        (_batch_norm): BatchNorm()\n",
       "      )\n",
       "      (cheap_operation): ConvBNLayer(\n",
       "        (_conv): Conv2D(26, 26, kernel_size=[3, 3], padding=1, groups=26, data_format=NCHW)\n",
       "        (_batch_norm): BatchNorm()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (_ghostbottleneck_5): GhostBottleneck(\n",
       "    (ghost_module_1): GhostModule(\n",
       "      (primary_conv): ConvBNLayer(\n",
       "        (_conv): Conv2D(52, 156, kernel_size=[1, 1], data_format=NCHW)\n",
       "        (_batch_norm): BatchNorm()\n",
       "      )\n",
       "      (cheap_operation): ConvBNLayer(\n",
       "        (_conv): Conv2D(156, 156, kernel_size=[3, 3], padding=1, groups=156, data_format=NCHW)\n",
       "        (_batch_norm): BatchNorm()\n",
       "      )\n",
       "    )\n",
       "    (depthwise_conv): ConvBNLayer(\n",
       "      (_conv): Conv2D(312, 312, kernel_size=[3, 3], stride=[2, 2], padding=1, groups=312, data_format=NCHW)\n",
       "      (_batch_norm): BatchNorm()\n",
       "    )\n",
       "    (ghost_module_2): GhostModule(\n",
       "      (primary_conv): ConvBNLayer(\n",
       "        (_conv): Conv2D(312, 52, kernel_size=[1, 1], data_format=NCHW)\n",
       "        (_batch_norm): BatchNorm()\n",
       "      )\n",
       "      (cheap_operation): ConvBNLayer(\n",
       "        (_conv): Conv2D(52, 52, kernel_size=[3, 3], padding=1, groups=52, data_format=NCHW)\n",
       "        (_batch_norm): BatchNorm()\n",
       "      )\n",
       "    )\n",
       "    (shortcut_depthwise): ConvBNLayer(\n",
       "      (_conv): Conv2D(52, 52, kernel_size=[3, 3], stride=[2, 2], padding=1, groups=52, data_format=NCHW)\n",
       "      (_batch_norm): BatchNorm()\n",
       "    )\n",
       "    (shortcut_conv): ConvBNLayer(\n",
       "      (_conv): Conv2D(52, 104, kernel_size=[1, 1], data_format=NCHW)\n",
       "      (_batch_norm): BatchNorm()\n",
       "    )\n",
       "  )\n",
       "  (_ghostbottleneck_6): GhostBottleneck(\n",
       "    (ghost_module_1): GhostModule(\n",
       "      (primary_conv): ConvBNLayer(\n",
       "        (_conv): Conv2D(104, 130, kernel_size=[1, 1], data_format=NCHW)\n",
       "        (_batch_norm): BatchNorm()\n",
       "      )\n",
       "      (cheap_operation): ConvBNLayer(\n",
       "        (_conv): Conv2D(130, 130, kernel_size=[3, 3], padding=1, groups=130, data_format=NCHW)\n",
       "        (_batch_norm): BatchNorm()\n",
       "      )\n",
       "    )\n",
       "    (ghost_module_2): GhostModule(\n",
       "      (primary_conv): ConvBNLayer(\n",
       "        (_conv): Conv2D(260, 52, kernel_size=[1, 1], data_format=NCHW)\n",
       "        (_batch_norm): BatchNorm()\n",
       "      )\n",
       "      (cheap_operation): ConvBNLayer(\n",
       "        (_conv): Conv2D(52, 52, kernel_size=[3, 3], padding=1, groups=52, data_format=NCHW)\n",
       "        (_batch_norm): BatchNorm()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (_ghostbottleneck_7): GhostBottleneck(\n",
       "    (ghost_module_1): GhostModule(\n",
       "      (primary_conv): ConvBNLayer(\n",
       "        (_conv): Conv2D(104, 120, kernel_size=[1, 1], data_format=NCHW)\n",
       "        (_batch_norm): BatchNorm()\n",
       "      )\n",
       "      (cheap_operation): ConvBNLayer(\n",
       "        (_conv): Conv2D(120, 120, kernel_size=[3, 3], padding=1, groups=120, data_format=NCHW)\n",
       "        (_batch_norm): BatchNorm()\n",
       "      )\n",
       "    )\n",
       "    (ghost_module_2): GhostModule(\n",
       "      (primary_conv): ConvBNLayer(\n",
       "        (_conv): Conv2D(240, 52, kernel_size=[1, 1], data_format=NCHW)\n",
       "        (_batch_norm): BatchNorm()\n",
       "      )\n",
       "      (cheap_operation): ConvBNLayer(\n",
       "        (_conv): Conv2D(52, 52, kernel_size=[3, 3], padding=1, groups=52, data_format=NCHW)\n",
       "        (_batch_norm): BatchNorm()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (_ghostbottleneck_8): GhostBottleneck(\n",
       "    (ghost_module_1): GhostModule(\n",
       "      (primary_conv): ConvBNLayer(\n",
       "        (_conv): Conv2D(104, 120, kernel_size=[1, 1], data_format=NCHW)\n",
       "        (_batch_norm): BatchNorm()\n",
       "      )\n",
       "      (cheap_operation): ConvBNLayer(\n",
       "        (_conv): Conv2D(120, 120, kernel_size=[3, 3], padding=1, groups=120, data_format=NCHW)\n",
       "        (_batch_norm): BatchNorm()\n",
       "      )\n",
       "    )\n",
       "    (ghost_module_2): GhostModule(\n",
       "      (primary_conv): ConvBNLayer(\n",
       "        (_conv): Conv2D(240, 52, kernel_size=[1, 1], data_format=NCHW)\n",
       "        (_batch_norm): BatchNorm()\n",
       "      )\n",
       "      (cheap_operation): ConvBNLayer(\n",
       "        (_conv): Conv2D(52, 52, kernel_size=[3, 3], padding=1, groups=52, data_format=NCHW)\n",
       "        (_batch_norm): BatchNorm()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (_ghostbottleneck_9): GhostBottleneck(\n",
       "    (ghost_module_1): GhostModule(\n",
       "      (primary_conv): ConvBNLayer(\n",
       "        (_conv): Conv2D(104, 312, kernel_size=[1, 1], data_format=NCHW)\n",
       "        (_batch_norm): BatchNorm()\n",
       "      )\n",
       "      (cheap_operation): ConvBNLayer(\n",
       "        (_conv): Conv2D(312, 312, kernel_size=[3, 3], padding=1, groups=312, data_format=NCHW)\n",
       "        (_batch_norm): BatchNorm()\n",
       "      )\n",
       "    )\n",
       "    (se_block): SEBlock(\n",
       "      (pool2d_gap): AdaptiveAvgPool2D(output_size=1)\n",
       "      (squeeze): Linear(in_features=624, out_features=156, dtype=float32)\n",
       "      (excitation): Linear(in_features=156, out_features=624, dtype=float32)\n",
       "    )\n",
       "    (ghost_module_2): GhostModule(\n",
       "      (primary_conv): ConvBNLayer(\n",
       "        (_conv): Conv2D(624, 72, kernel_size=[1, 1], data_format=NCHW)\n",
       "        (_batch_norm): BatchNorm()\n",
       "      )\n",
       "      (cheap_operation): ConvBNLayer(\n",
       "        (_conv): Conv2D(72, 72, kernel_size=[3, 3], padding=1, groups=72, data_format=NCHW)\n",
       "        (_batch_norm): BatchNorm()\n",
       "      )\n",
       "    )\n",
       "    (shortcut_depthwise): ConvBNLayer(\n",
       "      (_conv): Conv2D(104, 104, kernel_size=[3, 3], padding=1, groups=104, data_format=NCHW)\n",
       "      (_batch_norm): BatchNorm()\n",
       "    )\n",
       "    (shortcut_conv): ConvBNLayer(\n",
       "      (_conv): Conv2D(104, 144, kernel_size=[1, 1], data_format=NCHW)\n",
       "      (_batch_norm): BatchNorm()\n",
       "    )\n",
       "  )\n",
       "  (_ghostbottleneck_10): GhostBottleneck(\n",
       "    (ghost_module_1): GhostModule(\n",
       "      (primary_conv): ConvBNLayer(\n",
       "        (_conv): Conv2D(144, 436, kernel_size=[1, 1], data_format=NCHW)\n",
       "        (_batch_norm): BatchNorm()\n",
       "      )\n",
       "      (cheap_operation): ConvBNLayer(\n",
       "        (_conv): Conv2D(436, 436, kernel_size=[3, 3], padding=1, groups=436, data_format=NCHW)\n",
       "        (_batch_norm): BatchNorm()\n",
       "      )\n",
       "    )\n",
       "    (se_block): SEBlock(\n",
       "      (pool2d_gap): AdaptiveAvgPool2D(output_size=1)\n",
       "      (squeeze): Linear(in_features=872, out_features=218, dtype=float32)\n",
       "      (excitation): Linear(in_features=218, out_features=872, dtype=float32)\n",
       "    )\n",
       "    (ghost_module_2): GhostModule(\n",
       "      (primary_conv): ConvBNLayer(\n",
       "        (_conv): Conv2D(872, 72, kernel_size=[1, 1], data_format=NCHW)\n",
       "        (_batch_norm): BatchNorm()\n",
       "      )\n",
       "      (cheap_operation): ConvBNLayer(\n",
       "        (_conv): Conv2D(72, 72, kernel_size=[3, 3], padding=1, groups=72, data_format=NCHW)\n",
       "        (_batch_norm): BatchNorm()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (_ghostbottleneck_11): GhostBottleneck(\n",
       "    (ghost_module_1): GhostModule(\n",
       "      (primary_conv): ConvBNLayer(\n",
       "        (_conv): Conv2D(144, 436, kernel_size=[1, 1], data_format=NCHW)\n",
       "        (_batch_norm): BatchNorm()\n",
       "      )\n",
       "      (cheap_operation): ConvBNLayer(\n",
       "        (_conv): Conv2D(436, 436, kernel_size=[3, 3], padding=1, groups=436, data_format=NCHW)\n",
       "        (_batch_norm): BatchNorm()\n",
       "      )\n",
       "    )\n",
       "    (depthwise_conv): ConvBNLayer(\n",
       "      (_conv): Conv2D(872, 872, kernel_size=[5, 5], stride=[2, 2], padding=2, groups=872, data_format=NCHW)\n",
       "      (_batch_norm): BatchNorm()\n",
       "    )\n",
       "    (se_block): SEBlock(\n",
       "      (pool2d_gap): AdaptiveAvgPool2D(output_size=1)\n",
       "      (squeeze): Linear(in_features=872, out_features=218, dtype=float32)\n",
       "      (excitation): Linear(in_features=218, out_features=872, dtype=float32)\n",
       "    )\n",
       "    (ghost_module_2): GhostModule(\n",
       "      (primary_conv): ConvBNLayer(\n",
       "        (_conv): Conv2D(872, 104, kernel_size=[1, 1], data_format=NCHW)\n",
       "        (_batch_norm): BatchNorm()\n",
       "      )\n",
       "      (cheap_operation): ConvBNLayer(\n",
       "        (_conv): Conv2D(104, 104, kernel_size=[3, 3], padding=1, groups=104, data_format=NCHW)\n",
       "        (_batch_norm): BatchNorm()\n",
       "      )\n",
       "    )\n",
       "    (shortcut_depthwise): ConvBNLayer(\n",
       "      (_conv): Conv2D(144, 144, kernel_size=[5, 5], stride=[2, 2], padding=2, groups=144, data_format=NCHW)\n",
       "      (_batch_norm): BatchNorm()\n",
       "    )\n",
       "    (shortcut_conv): ConvBNLayer(\n",
       "      (_conv): Conv2D(144, 208, kernel_size=[1, 1], data_format=NCHW)\n",
       "      (_batch_norm): BatchNorm()\n",
       "    )\n",
       "  )\n",
       "  (_ghostbottleneck_12): GhostBottleneck(\n",
       "    (ghost_module_1): GhostModule(\n",
       "      (primary_conv): ConvBNLayer(\n",
       "        (_conv): Conv2D(208, 624, kernel_size=[1, 1], data_format=NCHW)\n",
       "        (_batch_norm): BatchNorm()\n",
       "      )\n",
       "      (cheap_operation): ConvBNLayer(\n",
       "        (_conv): Conv2D(624, 624, kernel_size=[3, 3], padding=1, groups=624, data_format=NCHW)\n",
       "        (_batch_norm): BatchNorm()\n",
       "      )\n",
       "    )\n",
       "    (ghost_module_2): GhostModule(\n",
       "      (primary_conv): ConvBNLayer(\n",
       "        (_conv): Conv2D(1248, 104, kernel_size=[1, 1], data_format=NCHW)\n",
       "        (_batch_norm): BatchNorm()\n",
       "      )\n",
       "      (cheap_operation): ConvBNLayer(\n",
       "        (_conv): Conv2D(104, 104, kernel_size=[3, 3], padding=1, groups=104, data_format=NCHW)\n",
       "        (_batch_norm): BatchNorm()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (_ghostbottleneck_13): GhostBottleneck(\n",
       "    (ghost_module_1): GhostModule(\n",
       "      (primary_conv): ConvBNLayer(\n",
       "        (_conv): Conv2D(208, 624, kernel_size=[1, 1], data_format=NCHW)\n",
       "        (_batch_norm): BatchNorm()\n",
       "      )\n",
       "      (cheap_operation): ConvBNLayer(\n",
       "        (_conv): Conv2D(624, 624, kernel_size=[3, 3], padding=1, groups=624, data_format=NCHW)\n",
       "        (_batch_norm): BatchNorm()\n",
       "      )\n",
       "    )\n",
       "    (se_block): SEBlock(\n",
       "      (pool2d_gap): AdaptiveAvgPool2D(output_size=1)\n",
       "      (squeeze): Linear(in_features=1248, out_features=312, dtype=float32)\n",
       "      (excitation): Linear(in_features=312, out_features=1248, dtype=float32)\n",
       "    )\n",
       "    (ghost_module_2): GhostModule(\n",
       "      (primary_conv): ConvBNLayer(\n",
       "        (_conv): Conv2D(1248, 104, kernel_size=[1, 1], data_format=NCHW)\n",
       "        (_batch_norm): BatchNorm()\n",
       "      )\n",
       "      (cheap_operation): ConvBNLayer(\n",
       "        (_conv): Conv2D(104, 104, kernel_size=[3, 3], padding=1, groups=104, data_format=NCHW)\n",
       "        (_batch_norm): BatchNorm()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (_ghostbottleneck_14): GhostBottleneck(\n",
       "    (ghost_module_1): GhostModule(\n",
       "      (primary_conv): ConvBNLayer(\n",
       "        (_conv): Conv2D(208, 624, kernel_size=[1, 1], data_format=NCHW)\n",
       "        (_batch_norm): BatchNorm()\n",
       "      )\n",
       "      (cheap_operation): ConvBNLayer(\n",
       "        (_conv): Conv2D(624, 624, kernel_size=[3, 3], padding=1, groups=624, data_format=NCHW)\n",
       "        (_batch_norm): BatchNorm()\n",
       "      )\n",
       "    )\n",
       "    (ghost_module_2): GhostModule(\n",
       "      (primary_conv): ConvBNLayer(\n",
       "        (_conv): Conv2D(1248, 104, kernel_size=[1, 1], data_format=NCHW)\n",
       "        (_batch_norm): BatchNorm()\n",
       "      )\n",
       "      (cheap_operation): ConvBNLayer(\n",
       "        (_conv): Conv2D(104, 104, kernel_size=[3, 3], padding=1, groups=104, data_format=NCHW)\n",
       "        (_batch_norm): BatchNorm()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (_ghostbottleneck_15): GhostBottleneck(\n",
       "    (ghost_module_1): GhostModule(\n",
       "      (primary_conv): ConvBNLayer(\n",
       "        (_conv): Conv2D(208, 624, kernel_size=[1, 1], data_format=NCHW)\n",
       "        (_batch_norm): BatchNorm()\n",
       "      )\n",
       "      (cheap_operation): ConvBNLayer(\n",
       "        (_conv): Conv2D(624, 624, kernel_size=[3, 3], padding=1, groups=624, data_format=NCHW)\n",
       "        (_batch_norm): BatchNorm()\n",
       "      )\n",
       "    )\n",
       "    (se_block): SEBlock(\n",
       "      (pool2d_gap): AdaptiveAvgPool2D(output_size=1)\n",
       "      (squeeze): Linear(in_features=1248, out_features=312, dtype=float32)\n",
       "      (excitation): Linear(in_features=312, out_features=1248, dtype=float32)\n",
       "    )\n",
       "    (ghost_module_2): GhostModule(\n",
       "      (primary_conv): ConvBNLayer(\n",
       "        (_conv): Conv2D(1248, 104, kernel_size=[1, 1], data_format=NCHW)\n",
       "        (_batch_norm): BatchNorm()\n",
       "      )\n",
       "      (cheap_operation): ConvBNLayer(\n",
       "        (_conv): Conv2D(104, 104, kernel_size=[3, 3], padding=1, groups=104, data_format=NCHW)\n",
       "        (_batch_norm): BatchNorm()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv_last): ConvBNLayer(\n",
       "    (_conv): Conv2D(208, 1248, kernel_size=[1, 1], data_format=NCHW)\n",
       "    (_batch_norm): BatchNorm()\n",
       "  )\n",
       "  (pool2d_gap): AdaptiveAvgPool2D(output_size=1)\n",
       "  (fc_0): ConvBNLayer(\n",
       "    (_conv): Conv2D(1248, 1280, kernel_size=[1, 1], data_format=NCHW)\n",
       "    (_batch_norm): BatchNorm()\n",
       "  )\n",
       "  (dropout): Dropout(p=0.2, axis=None, mode=upscale_in_train)\n",
       "  (fc_1): Linear(in_features=1280, out_features=1000, dtype=float32)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "346910cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GhostNet' object has no attribute 'cpu_predictor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2366/2003235752.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu_predictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_input_tensor_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"input shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mean:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_pretrained_images_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"std:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_pretrained_images_std\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/paddle/fluid/dygraph/layers.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1232\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0m_convert_into_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_buffers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1233\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_buffers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1234\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GhostNet' object has no attribute 'cpu_predictor'"
     ]
    }
   ],
   "source": [
    "input_shape = list(model.cpu_predictor.get_input_tensor_shape().values())\n",
    "print(\"input shape:\", input_shape)\n",
    "print(\"mean:\", model.get_pretrained_images_mean())\n",
    "print(\"std:\", model.get_pretrained_images_std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a788b17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5711, -0.5841, -0.1849,  ..., -1.0731,  0.5039, -0.6233],\n",
      "        [ 0.3154, -0.1095,  0.1844,  ..., -0.2647,  0.2654, -0.0175],\n",
      "        [ 0.6115,  0.8285, -0.4633,  ...,  0.5001,  0.5750, -0.0180],\n",
      "        ...,\n",
      "        [ 0.3146, -0.3866,  0.2066,  ...,  0.0082,  0.0571,  0.4113],\n",
      "        [ 0.2931, -0.1015, -0.0362,  ..., -0.7712,  0.6064,  0.0324],\n",
      "        [ 0.2098, -0.7677,  0.4077,  ...,  0.3126,  0.2986, -0.0761]],\n",
      "       grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from ghost_net import ghost_net\n",
    "\n",
    "model = ghost_net(width_mult=1.0)\n",
    "input = torch.randn(32,3,224,224)\n",
    "y = model(input)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b8a573d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GhostNet(\n",
       "  (features): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): GhostBottleneck(\n",
       "      (conv): Sequential(\n",
       "        (0): GhostModule(\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)\n",
       "            (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential()\n",
       "        (2): Sequential()\n",
       "        (3): GhostModule(\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)\n",
       "            (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): GhostBottleneck(\n",
       "      (conv): Sequential(\n",
       "        (0): GhostModule(\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(16, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
       "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): Sequential()\n",
       "        )\n",
       "        (2): Sequential()\n",
       "        (3): GhostModule(\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=12, bias=False)\n",
       "            (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (shortcut): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16, bias=False)\n",
       "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): Sequential()\n",
       "        )\n",
       "        (1): Conv2d(16, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): GhostBottleneck(\n",
       "      (conv): Sequential(\n",
       "        (0): GhostModule(\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(24, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=36, bias=False)\n",
       "            (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential()\n",
       "        (2): Sequential()\n",
       "        (3): GhostModule(\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(72, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=12, bias=False)\n",
       "            (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (4): GhostBottleneck(\n",
       "      (conv): Sequential(\n",
       "        (0): GhostModule(\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(24, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=36, bias=False)\n",
       "            (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)\n",
       "          (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): Sequential()\n",
       "        )\n",
       "        (2): SELayer(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc): Sequential(\n",
       "            (0): Linear(in_features=72, out_features=18, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Linear(in_features=18, out_features=72, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (3): GhostModule(\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(72, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=20, bias=False)\n",
       "            (1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (shortcut): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): Conv2d(24, 24, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=24, bias=False)\n",
       "          (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): Sequential()\n",
       "        )\n",
       "        (1): Conv2d(24, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): GhostBottleneck(\n",
       "      (conv): Sequential(\n",
       "        (0): GhostModule(\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(40, 60, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(60, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=60, bias=False)\n",
       "            (1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential()\n",
       "        (2): SELayer(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc): Sequential(\n",
       "            (0): Linear(in_features=120, out_features=30, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Linear(in_features=30, out_features=120, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (3): GhostModule(\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(120, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=20, bias=False)\n",
       "            (1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (6): GhostBottleneck(\n",
       "      (conv): Sequential(\n",
       "        (0): GhostModule(\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(120, 120, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=120, bias=False)\n",
       "            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
       "          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): Sequential()\n",
       "        )\n",
       "        (2): Sequential()\n",
       "        (3): GhostModule(\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)\n",
       "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (shortcut): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=40, bias=False)\n",
       "          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): Sequential()\n",
       "        )\n",
       "        (1): Conv2d(40, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): GhostBottleneck(\n",
       "      (conv): Sequential(\n",
       "        (0): GhostModule(\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(80, 100, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(100, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=100, bias=False)\n",
       "            (1): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential()\n",
       "        (2): Sequential()\n",
       "        (3): GhostModule(\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(200, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)\n",
       "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (8): GhostBottleneck(\n",
       "      (conv): Sequential(\n",
       "        (0): GhostModule(\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(80, 92, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(92, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(92, 92, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=92, bias=False)\n",
       "            (1): BatchNorm2d(92, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential()\n",
       "        (2): Sequential()\n",
       "        (3): GhostModule(\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(184, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)\n",
       "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (9): GhostBottleneck(\n",
       "      (conv): Sequential(\n",
       "        (0): GhostModule(\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(80, 92, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(92, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(92, 92, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=92, bias=False)\n",
       "            (1): BatchNorm2d(92, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential()\n",
       "        (2): Sequential()\n",
       "        (3): GhostModule(\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(184, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)\n",
       "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (10): GhostBottleneck(\n",
       "      (conv): Sequential(\n",
       "        (0): GhostModule(\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential()\n",
       "        (2): SELayer(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc): Sequential(\n",
       "            (0): Linear(in_features=480, out_features=120, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Linear(in_features=120, out_features=480, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (3): GhostModule(\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(480, 56, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(56, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=56, bias=False)\n",
       "            (1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (shortcut): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): Sequential()\n",
       "        )\n",
       "        (1): Conv2d(80, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (11): GhostBottleneck(\n",
       "      (conv): Sequential(\n",
       "        (0): GhostModule(\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(112, 336, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(336, 336, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=336, bias=False)\n",
       "            (1): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential()\n",
       "        (2): SELayer(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc): Sequential(\n",
       "            (0): Linear(in_features=672, out_features=168, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Linear(in_features=168, out_features=672, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (3): GhostModule(\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(672, 56, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(56, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=56, bias=False)\n",
       "            (1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (12): GhostBottleneck(\n",
       "      (conv): Sequential(\n",
       "        (0): GhostModule(\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(112, 336, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(336, 336, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=336, bias=False)\n",
       "            (1): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
       "          (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): Sequential()\n",
       "        )\n",
       "        (2): SELayer(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc): Sequential(\n",
       "            (0): Linear(in_features=672, out_features=168, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Linear(in_features=168, out_features=672, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (3): GhostModule(\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(672, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (shortcut): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): Conv2d(112, 112, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=112, bias=False)\n",
       "          (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): Sequential()\n",
       "        )\n",
       "        (1): Conv2d(112, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (13): GhostBottleneck(\n",
       "      (conv): Sequential(\n",
       "        (0): GhostModule(\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(160, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential()\n",
       "        (2): Sequential()\n",
       "        (3): GhostModule(\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(960, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (14): GhostBottleneck(\n",
       "      (conv): Sequential(\n",
       "        (0): GhostModule(\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(160, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential()\n",
       "        (2): SELayer(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc): Sequential(\n",
       "            (0): Linear(in_features=960, out_features=240, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Linear(in_features=240, out_features=960, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (3): GhostModule(\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(960, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (15): GhostBottleneck(\n",
       "      (conv): Sequential(\n",
       "        (0): GhostModule(\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(160, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential()\n",
       "        (2): Sequential()\n",
       "        (3): GhostModule(\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(960, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (16): GhostBottleneck(\n",
       "      (conv): Sequential(\n",
       "        (0): GhostModule(\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(160, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential()\n",
       "        (2): SELayer(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc): Sequential(\n",
       "            (0): Linear(in_features=960, out_features=240, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Linear(in_features=240, out_features=960, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (3): GhostModule(\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(960, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (squeeze): Sequential(\n",
       "    (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=960, out_features=1280, bias=False)\n",
       "    (1): BatchNorm1d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.2, inplace=False)\n",
       "    (4): Linear(in_features=1280, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6b7eda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ghostnetv2_torch\n",
    "from timm.models import create_model, resume_checkpoint\n",
    "\n",
    "model = create_model('ghostnetv2',num_classes=1000,width=1.0,dropout=0.1,args=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed784b15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GhostNetV2(\n",
       "  (conv_stem): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act1): ReLU(inplace=True)\n",
       "  (blocks): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): GhostBottleneckV2(\n",
       "        (ghost1): GhostModuleV2(\n",
       "          (gate_fn): Sigmoid()\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)\n",
       "            (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (ghost2): GhostModuleV2(\n",
       "          (gate_fn): Sigmoid()\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)\n",
       "            (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "        )\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): GhostBottleneckV2(\n",
       "        (ghost1): GhostModuleV2(\n",
       "          (gate_fn): Sigmoid()\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(16, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
       "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)\n",
       "        (bn_dw): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (ghost2): GhostModuleV2(\n",
       "          (gate_fn): Sigmoid()\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=12, bias=False)\n",
       "            (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "        )\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16, bias=False)\n",
       "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): Conv2d(16, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): GhostBottleneckV2(\n",
       "        (ghost1): GhostModuleV2(\n",
       "          (gate_fn): Sigmoid()\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(24, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=36, bias=False)\n",
       "            (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (short_conv): Sequential(\n",
       "            (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Conv2d(72, 72, kernel_size=(1, 5), stride=(1, 1), padding=(0, 2), groups=72, bias=False)\n",
       "            (3): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (4): Conv2d(72, 72, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0), groups=72, bias=False)\n",
       "            (5): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (ghost2): GhostModuleV2(\n",
       "          (gate_fn): Sigmoid()\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(72, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=12, bias=False)\n",
       "            (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "        )\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): GhostBottleneckV2(\n",
       "        (ghost1): GhostModuleV2(\n",
       "          (gate_fn): Sigmoid()\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(24, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=36, bias=False)\n",
       "            (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (short_conv): Sequential(\n",
       "            (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Conv2d(72, 72, kernel_size=(1, 5), stride=(1, 1), padding=(0, 2), groups=72, bias=False)\n",
       "            (3): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (4): Conv2d(72, 72, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0), groups=72, bias=False)\n",
       "            (5): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (conv_dw): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)\n",
       "        (bn_dw): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (conv_reduce): Conv2d(72, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): ReLU(inplace=True)\n",
       "          (conv_expand): Conv2d(20, 72, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (ghost2): GhostModuleV2(\n",
       "          (gate_fn): Sigmoid()\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(72, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=20, bias=False)\n",
       "            (1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "        )\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(24, 24, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=24, bias=False)\n",
       "          (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): Conv2d(24, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): GhostBottleneckV2(\n",
       "        (ghost1): GhostModuleV2(\n",
       "          (gate_fn): Sigmoid()\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(40, 60, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(60, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=60, bias=False)\n",
       "            (1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (short_conv): Sequential(\n",
       "            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Conv2d(120, 120, kernel_size=(1, 5), stride=(1, 1), padding=(0, 2), groups=120, bias=False)\n",
       "            (3): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (4): Conv2d(120, 120, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0), groups=120, bias=False)\n",
       "            (5): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (conv_reduce): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): ReLU(inplace=True)\n",
       "          (conv_expand): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (ghost2): GhostModuleV2(\n",
       "          (gate_fn): Sigmoid()\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(120, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=20, bias=False)\n",
       "            (1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "        )\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): GhostBottleneckV2(\n",
       "        (ghost1): GhostModuleV2(\n",
       "          (gate_fn): Sigmoid()\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(120, 120, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=120, bias=False)\n",
       "            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (short_conv): Sequential(\n",
       "            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Conv2d(240, 240, kernel_size=(1, 5), stride=(1, 1), padding=(0, 2), groups=240, bias=False)\n",
       "            (3): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (4): Conv2d(240, 240, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0), groups=240, bias=False)\n",
       "            (5): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (conv_dw): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
       "        (bn_dw): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (ghost2): GhostModuleV2(\n",
       "          (gate_fn): Sigmoid()\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)\n",
       "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "        )\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=40, bias=False)\n",
       "          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): Conv2d(40, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): GhostBottleneckV2(\n",
       "        (ghost1): GhostModuleV2(\n",
       "          (gate_fn): Sigmoid()\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(80, 100, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(100, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=100, bias=False)\n",
       "            (1): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (short_conv): Sequential(\n",
       "            (0): Conv2d(80, 200, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Conv2d(200, 200, kernel_size=(1, 5), stride=(1, 1), padding=(0, 2), groups=200, bias=False)\n",
       "            (3): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (4): Conv2d(200, 200, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0), groups=200, bias=False)\n",
       "            (5): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (ghost2): GhostModuleV2(\n",
       "          (gate_fn): Sigmoid()\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(200, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)\n",
       "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "        )\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "      (1): GhostBottleneckV2(\n",
       "        (ghost1): GhostModuleV2(\n",
       "          (gate_fn): Sigmoid()\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(80, 92, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(92, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(92, 92, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=92, bias=False)\n",
       "            (1): BatchNorm2d(92, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (short_conv): Sequential(\n",
       "            (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(184, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Conv2d(184, 184, kernel_size=(1, 5), stride=(1, 1), padding=(0, 2), groups=184, bias=False)\n",
       "            (3): BatchNorm2d(184, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (4): Conv2d(184, 184, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0), groups=184, bias=False)\n",
       "            (5): BatchNorm2d(184, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (ghost2): GhostModuleV2(\n",
       "          (gate_fn): Sigmoid()\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(184, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)\n",
       "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "        )\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "      (2): GhostBottleneckV2(\n",
       "        (ghost1): GhostModuleV2(\n",
       "          (gate_fn): Sigmoid()\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(80, 92, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(92, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(92, 92, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=92, bias=False)\n",
       "            (1): BatchNorm2d(92, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (short_conv): Sequential(\n",
       "            (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(184, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Conv2d(184, 184, kernel_size=(1, 5), stride=(1, 1), padding=(0, 2), groups=184, bias=False)\n",
       "            (3): BatchNorm2d(184, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (4): Conv2d(184, 184, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0), groups=184, bias=False)\n",
       "            (5): BatchNorm2d(184, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (ghost2): GhostModuleV2(\n",
       "          (gate_fn): Sigmoid()\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(184, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)\n",
       "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "        )\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "      (3): GhostBottleneckV2(\n",
       "        (ghost1): GhostModuleV2(\n",
       "          (gate_fn): Sigmoid()\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (short_conv): Sequential(\n",
       "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Conv2d(480, 480, kernel_size=(1, 5), stride=(1, 1), padding=(0, 2), groups=480, bias=False)\n",
       "            (3): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (4): Conv2d(480, 480, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0), groups=480, bias=False)\n",
       "            (5): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (conv_reduce): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): ReLU(inplace=True)\n",
       "          (conv_expand): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (ghost2): GhostModuleV2(\n",
       "          (gate_fn): Sigmoid()\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(480, 56, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(56, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=56, bias=False)\n",
       "            (1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "        )\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): Conv2d(80, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (4): GhostBottleneckV2(\n",
       "        (ghost1): GhostModuleV2(\n",
       "          (gate_fn): Sigmoid()\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(112, 336, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(336, 336, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=336, bias=False)\n",
       "            (1): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (short_conv): Sequential(\n",
       "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Conv2d(672, 672, kernel_size=(1, 5), stride=(1, 1), padding=(0, 2), groups=672, bias=False)\n",
       "            (3): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (4): Conv2d(672, 672, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0), groups=672, bias=False)\n",
       "            (5): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (conv_reduce): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): ReLU(inplace=True)\n",
       "          (conv_expand): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (ghost2): GhostModuleV2(\n",
       "          (gate_fn): Sigmoid()\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(672, 56, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(56, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=56, bias=False)\n",
       "            (1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "        )\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): GhostBottleneckV2(\n",
       "        (ghost1): GhostModuleV2(\n",
       "          (gate_fn): Sigmoid()\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(112, 336, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(336, 336, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=336, bias=False)\n",
       "            (1): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (short_conv): Sequential(\n",
       "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Conv2d(672, 672, kernel_size=(1, 5), stride=(1, 1), padding=(0, 2), groups=672, bias=False)\n",
       "            (3): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (4): Conv2d(672, 672, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0), groups=672, bias=False)\n",
       "            (5): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (conv_dw): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
       "        (bn_dw): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (conv_reduce): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): ReLU(inplace=True)\n",
       "          (conv_expand): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (ghost2): GhostModuleV2(\n",
       "          (gate_fn): Sigmoid()\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(672, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "        )\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(112, 112, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=112, bias=False)\n",
       "          (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): Conv2d(112, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): Sequential(\n",
       "      (0): GhostBottleneckV2(\n",
       "        (ghost1): GhostModuleV2(\n",
       "          (gate_fn): Sigmoid()\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(160, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (short_conv): Sequential(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Conv2d(960, 960, kernel_size=(1, 5), stride=(1, 1), padding=(0, 2), groups=960, bias=False)\n",
       "            (3): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (4): Conv2d(960, 960, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0), groups=960, bias=False)\n",
       "            (5): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (ghost2): GhostModuleV2(\n",
       "          (gate_fn): Sigmoid()\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(960, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "        )\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "      (1): GhostBottleneckV2(\n",
       "        (ghost1): GhostModuleV2(\n",
       "          (gate_fn): Sigmoid()\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(160, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (short_conv): Sequential(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Conv2d(960, 960, kernel_size=(1, 5), stride=(1, 1), padding=(0, 2), groups=960, bias=False)\n",
       "            (3): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (4): Conv2d(960, 960, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0), groups=960, bias=False)\n",
       "            (5): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (conv_reduce): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): ReLU(inplace=True)\n",
       "          (conv_expand): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (ghost2): GhostModuleV2(\n",
       "          (gate_fn): Sigmoid()\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(960, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "        )\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "      (2): GhostBottleneckV2(\n",
       "        (ghost1): GhostModuleV2(\n",
       "          (gate_fn): Sigmoid()\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(160, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (short_conv): Sequential(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Conv2d(960, 960, kernel_size=(1, 5), stride=(1, 1), padding=(0, 2), groups=960, bias=False)\n",
       "            (3): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (4): Conv2d(960, 960, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0), groups=960, bias=False)\n",
       "            (5): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (ghost2): GhostModuleV2(\n",
       "          (gate_fn): Sigmoid()\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(960, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "        )\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "      (3): GhostBottleneckV2(\n",
       "        (ghost1): GhostModuleV2(\n",
       "          (gate_fn): Sigmoid()\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(160, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (short_conv): Sequential(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Conv2d(960, 960, kernel_size=(1, 5), stride=(1, 1), padding=(0, 2), groups=960, bias=False)\n",
       "            (3): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (4): Conv2d(960, 960, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0), groups=960, bias=False)\n",
       "            (5): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (conv_reduce): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): ReLU(inplace=True)\n",
       "          (conv_expand): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (ghost2): GhostModuleV2(\n",
       "          (gate_fn): Sigmoid()\n",
       "          (primary_conv): Sequential(\n",
       "            (0): Conv2d(960, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "          (cheap_operation): Sequential(\n",
       "            (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Sequential()\n",
       "          )\n",
       "        )\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "    )\n",
       "    (9): Sequential(\n",
       "      (0): ConvBnAct(\n",
       "        (conv): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (global_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (conv_head): Conv2d(960, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (act2): ReLU(inplace=True)\n",
       "  (classifier): Linear(in_features=1280, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67ee116a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for GhostNetV2:\n\tUnexpected key(s) in state_dict: \"total_ops\", \"total_params\", \"blocks.0.0.total_ops\", \"blocks.0.0.total_params\", \"blocks.0.0.ghost1.total_ops\", \"blocks.0.0.ghost1.total_params\", \"blocks.0.0.ghost1.gate_fn.total_ops\", \"blocks.0.0.ghost1.gate_fn.total_params\", \"blocks.0.0.ghost2.total_ops\", \"blocks.0.0.ghost2.total_params\", \"blocks.0.0.ghost2.gate_fn.total_ops\", \"blocks.0.0.ghost2.gate_fn.total_params\", \"blocks.1.0.total_ops\", \"blocks.1.0.total_params\", \"blocks.1.0.ghost1.total_ops\", \"blocks.1.0.ghost1.total_params\", \"blocks.1.0.ghost1.gate_fn.total_ops\", \"blocks.1.0.ghost1.gate_fn.total_params\", \"blocks.1.0.ghost2.total_ops\", \"blocks.1.0.ghost2.total_params\", \"blocks.1.0.ghost2.gate_fn.total_ops\", \"blocks.1.0.ghost2.gate_fn.total_params\", \"blocks.2.0.total_ops\", \"blocks.2.0.total_params\", \"blocks.2.0.ghost1.total_ops\", \"blocks.2.0.ghost1.total_params\", \"blocks.2.0.ghost1.gate_fn.total_ops\", \"blocks.2.0.ghost1.gate_fn.total_params\", \"blocks.2.0.ghost2.total_ops\", \"blocks.2.0.ghost2.total_params\", \"blocks.2.0.ghost2.gate_fn.total_ops\", \"blocks.2.0.ghost2.gate_fn.total_params\", \"blocks.3.0.total_ops\", \"blocks.3.0.total_params\", \"blocks.3.0.ghost1.total_ops\", \"blocks.3.0.ghost1.total_params\", \"blocks.3.0.ghost1.gate_fn.total_ops\", \"blocks.3.0.ghost1.gate_fn.total_params\", \"blocks.3.0.se.total_ops\", \"blocks.3.0.se.total_params\", \"blocks.3.0.ghost2.total_ops\", \"blocks.3.0.ghost2.total_params\", \"blocks.3.0.ghost2.gate_fn.total_ops\", \"blocks.3.0.ghost2.gate_fn.total_params\", \"blocks.4.0.total_ops\", \"blocks.4.0.total_params\", \"blocks.4.0.ghost1.total_ops\", \"blocks.4.0.ghost1.total_params\", \"blocks.4.0.ghost1.gate_fn.total_ops\", \"blocks.4.0.ghost1.gate_fn.total_params\", \"blocks.4.0.se.total_ops\", \"blocks.4.0.se.total_params\", \"blocks.4.0.ghost2.total_ops\", \"blocks.4.0.ghost2.total_params\", \"blocks.4.0.ghost2.gate_fn.total_ops\", \"blocks.4.0.ghost2.gate_fn.total_params\", \"blocks.5.0.total_ops\", \"blocks.5.0.total_params\", \"blocks.5.0.ghost1.total_ops\", \"blocks.5.0.ghost1.total_params\", \"blocks.5.0.ghost1.gate_fn.total_ops\", \"blocks.5.0.ghost1.gate_fn.total_params\", \"blocks.5.0.ghost2.total_ops\", \"blocks.5.0.ghost2.total_params\", \"blocks.5.0.ghost2.gate_fn.total_ops\", \"blocks.5.0.ghost2.gate_fn.total_params\", \"blocks.6.0.total_ops\", \"blocks.6.0.total_params\", \"blocks.6.0.ghost1.total_ops\", \"blocks.6.0.ghost1.total_params\", \"blocks.6.0.ghost1.gate_fn.total_ops\", \"blocks.6.0.ghost1.gate_fn.total_params\", \"blocks.6.0.ghost2.total_ops\", \"blocks.6.0.ghost2.total_params\", \"blocks.6.0.ghost2.gate_fn.total_ops\", \"blocks.6.0.ghost2.gate_fn.total_params\", \"blocks.6.1.total_ops\", \"blocks.6.1.total_params\", \"blocks.6.1.ghost1.total_ops\", \"blocks.6.1.ghost1.total_params\", \"blocks.6.1.ghost1.gate_fn.total_ops\", \"blocks.6.1.ghost1.gate_fn.total_params\", \"blocks.6.1.ghost2.total_ops\", \"blocks.6.1.ghost2.total_params\", \"blocks.6.1.ghost2.gate_fn.total_ops\", \"blocks.6.1.ghost2.gate_fn.total_params\", \"blocks.6.2.total_ops\", \"blocks.6.2.total_params\", \"blocks.6.2.ghost1.total_ops\", \"blocks.6.2.ghost1.total_params\", \"blocks.6.2.ghost1.gate_fn.total_ops\", \"blocks.6.2.ghost1.gate_fn.total_params\", \"blocks.6.2.ghost2.total_ops\", \"blocks.6.2.ghost2.total_params\", \"blocks.6.2.ghost2.gate_fn.total_ops\", \"blocks.6.2.ghost2.gate_fn.total_params\", \"blocks.6.3.total_ops\", \"blocks.6.3.total_params\", \"blocks.6.3.ghost1.total_ops\", \"blocks.6.3.ghost1.total_params\", \"blocks.6.3.ghost1.gate_fn.total_ops\", \"blocks.6.3.ghost1.gate_fn.total_params\", \"blocks.6.3.se.total_ops\", \"blocks.6.3.se.total_params\", \"blocks.6.3.ghost2.total_ops\", \"blocks.6.3.ghost2.total_params\", \"blocks.6.3.ghost2.gate_fn.total_ops\", \"blocks.6.3.ghost2.gate_fn.total_params\", \"blocks.6.4.total_ops\", \"blocks.6.4.total_params\", \"blocks.6.4.ghost1.total_ops\", \"blocks.6.4.ghost1.total_params\", \"blocks.6.4.ghost1.gate_fn.total_ops\", \"blocks.6.4.ghost1.gate_fn.total_params\", \"blocks.6.4.se.total_ops\", \"blocks.6.4.se.total_params\", \"blocks.6.4.ghost2.total_ops\", \"blocks.6.4.ghost2.total_params\", \"blocks.6.4.ghost2.gate_fn.total_ops\", \"blocks.6.4.ghost2.gate_fn.total_params\", \"blocks.7.0.total_ops\", \"blocks.7.0.total_params\", \"blocks.7.0.ghost1.total_ops\", \"blocks.7.0.ghost1.total_params\", \"blocks.7.0.ghost1.gate_fn.total_ops\", \"blocks.7.0.ghost1.gate_fn.total_params\", \"blocks.7.0.se.total_ops\", \"blocks.7.0.se.total_params\", \"blocks.7.0.ghost2.total_ops\", \"blocks.7.0.ghost2.total_params\", \"blocks.7.0.ghost2.gate_fn.total_ops\", \"blocks.7.0.ghost2.gate_fn.total_params\", \"blocks.8.0.total_ops\", \"blocks.8.0.total_params\", \"blocks.8.0.ghost1.total_ops\", \"blocks.8.0.ghost1.total_params\", \"blocks.8.0.ghost1.gate_fn.total_ops\", \"blocks.8.0.ghost1.gate_fn.total_params\", \"blocks.8.0.ghost2.total_ops\", \"blocks.8.0.ghost2.total_params\", \"blocks.8.0.ghost2.gate_fn.total_ops\", \"blocks.8.0.ghost2.gate_fn.total_params\", \"blocks.8.1.total_ops\", \"blocks.8.1.total_params\", \"blocks.8.1.ghost1.total_ops\", \"blocks.8.1.ghost1.total_params\", \"blocks.8.1.ghost1.gate_fn.total_ops\", \"blocks.8.1.ghost1.gate_fn.total_params\", \"blocks.8.1.se.total_ops\", \"blocks.8.1.se.total_params\", \"blocks.8.1.ghost2.total_ops\", \"blocks.8.1.ghost2.total_params\", \"blocks.8.1.ghost2.gate_fn.total_ops\", \"blocks.8.1.ghost2.gate_fn.total_params\", \"blocks.8.2.total_ops\", \"blocks.8.2.total_params\", \"blocks.8.2.ghost1.total_ops\", \"blocks.8.2.ghost1.total_params\", \"blocks.8.2.ghost1.gate_fn.total_ops\", \"blocks.8.2.ghost1.gate_fn.total_params\", \"blocks.8.2.ghost2.total_ops\", \"blocks.8.2.ghost2.total_params\", \"blocks.8.2.ghost2.gate_fn.total_ops\", \"blocks.8.2.ghost2.gate_fn.total_params\", \"blocks.8.3.total_ops\", \"blocks.8.3.total_params\", \"blocks.8.3.ghost1.total_ops\", \"blocks.8.3.ghost1.total_params\", \"blocks.8.3.ghost1.gate_fn.total_ops\", \"blocks.8.3.ghost1.gate_fn.total_params\", \"blocks.8.3.se.total_ops\", \"blocks.8.3.se.total_params\", \"blocks.8.3.ghost2.total_ops\", \"blocks.8.3.ghost2.total_params\", \"blocks.8.3.ghost2.gate_fn.total_ops\", \"blocks.8.3.ghost2.gate_fn.total_params\", \"blocks.9.0.total_ops\", \"blocks.9.0.total_params\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_165/1722073472.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# optionally resume from a checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresume_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresume_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ck_ghostnetv2_10.pth.tar\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'optimizer'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresume_state\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_rank\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/timm/models/_helpers.py\u001b[0m in \u001b[0;36mresume_checkpoint\u001b[0;34m(model, checkpoint_path, optimizer, loss_scaler, log_info)\u001b[0m\n\u001b[1;32m    140\u001b[0m                 \u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loaded checkpoint '{}' (epoch {})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlog_info\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loaded checkpoint '{}'\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1405\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1406\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   1407\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   1408\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for GhostNetV2:\n\tUnexpected key(s) in state_dict: \"total_ops\", \"total_params\", \"blocks.0.0.total_ops\", \"blocks.0.0.total_params\", \"blocks.0.0.ghost1.total_ops\", \"blocks.0.0.ghost1.total_params\", \"blocks.0.0.ghost1.gate_fn.total_ops\", \"blocks.0.0.ghost1.gate_fn.total_params\", \"blocks.0.0.ghost2.total_ops\", \"blocks.0.0.ghost2.total_params\", \"blocks.0.0.ghost2.gate_fn.total_ops\", \"blocks.0.0.ghost2.gate_fn.total_params\", \"blocks.1.0.total_ops\", \"blocks.1.0.total_params\", \"blocks.1.0.ghost1.total_ops\", \"blocks.1.0.ghost1.total_params\", \"blocks.1.0.ghost1.gate_fn.total_ops\", \"blocks.1.0.ghost1.gate_fn.total_params\", \"blocks.1.0.ghost2.total_ops\", \"blocks.1.0.ghost2.total_params\", \"blocks.1.0.ghost2.gate_fn.total_ops\", \"blocks.1.0.ghost2.gate_fn.total_params\", \"blocks.2.0.total_ops\", \"blocks.2.0.total_params\", \"blocks.2.0.ghost1.total_ops\", \"blocks.2.0.ghost1.total_params\", \"blocks.2.0.ghost1.gate_fn.total_ops\", \"blocks.2.0.ghost1.gate_fn.total_params\", \"blocks.2.0.ghost2.total_ops\", \"blocks.2.0.ghost2.total_params\", \"blocks.2.0.ghost2.gate_fn.total_ops\", \"blocks.2.0.ghost2.gate_fn.total_params\", \"blocks.3.0.total_ops\", \"blocks.3.0.total_params\", \"blocks.3.0.ghost1.total_ops\", \"blocks.3.0.ghost1.total_params\", \"blocks.3.0.ghost1.gate_fn.total_ops\", \"blocks.3.0.ghost1.gate_fn.total_params\", \"blocks.3.0.se.total_ops\", \"blocks.3.0.se.total_params\", \"blocks.3.0.ghost2.total_ops\", \"blocks.3.0.ghost2.total_params\", \"blocks.3.0.ghost2.gate_fn.total_ops\", \"blocks.3.0.ghost2.gate_fn.total_params\", \"blocks.4.0.total_ops\", \"blocks.4.0.total_params\", \"blocks.4.0.ghost1.total_ops\", \"blocks.4.0.ghost1.total_params\", \"blocks.4.0.ghost1.gate_fn.total_ops\", \"blocks.4.0.ghost1.gate_fn.total_params\", \"blocks.4.0.se.total_ops\", \"blocks.4.0.se.total_params\", \"blocks.4.0.ghost2.total_ops\", \"blocks.4.0.ghost2.total_params\", \"blocks.4.0.ghost2.gate_fn.total_ops\", \"blocks.4.0.ghost2.gate_fn.total_params\", \"blocks.5.0.total_ops\", \"blocks.5.0.total_params\", \"blocks.5.0.ghost1.total_ops\", \"blocks.5.0.ghost1.total_params\", \"blocks.5.0.ghost1.gate_fn.total_ops\", \"blocks.5.0.ghost1.gate_fn.total_params\", \"blocks.5.0.ghost2.total_ops\", \"blocks.5.0.ghost2.total_params\", \"blocks.5.0.ghost2.gate_fn.total_ops\", \"blocks.5.0.ghost2.gate_fn.total_params\", \"blocks.6.0.total_ops\", \"blocks.6.0.total_params\", \"blocks.6.0.ghost1.total_ops\", \"blocks.6.0.ghost1.total_params\", \"blocks.6.0.ghost1.gate_fn.total_ops\", \"blocks.6.0.ghost1.gate_fn.total_params\", \"blocks.6.0.ghost2.total_ops\", \"blocks.6.0.ghost2.total_params\", \"blocks.6.0.ghost2.gate_fn.total_ops\", \"blocks.6.0.ghost2.gate_fn.total_params\", \"blocks.6.1.total_ops\", \"blocks.6.1.total_params\", \"blocks.6.1.ghost1.total_ops\", \"blocks.6.1.ghost1.total_params\", \"blocks.6.1.ghost1.gate_fn.total_ops\", \"blocks.6.1.ghost1.gate_fn.total_params\", \"blocks.6.1.ghost2.total_ops\", \"blocks.6.1.ghost2.total_params\", \"blocks.6.1.ghost2.gate_fn.total_ops\", \"blocks.6.1.ghost2.gate_fn.total_params\", \"blocks.6.2.total_ops\", \"blocks.6.2.total_params\", \"blocks.6.2.ghost1.total_ops\", \"blocks.6.2.ghost1.total_params\", \"blocks.6.2.ghost1.gate_fn.total_ops\", \"blocks.6.2.ghost1.gate_fn.total_params\", \"blocks.6.2.ghost2.total_ops\", \"blocks.6.2.ghost2.total_params\", \"blocks.6.2.ghost2.gate_fn.total_ops\", \"blocks.6.2.ghost2.gate_fn.total_params\", \"blocks.6.3.total_ops\", \"blocks.6.3.total_params\", \"blocks.6.3.ghost1.total_ops\", \"blocks.6.3.ghost1.total_params\", \"blocks.6.3.ghost1.gate_fn.total_ops\", \"blocks.6.3.ghost1.gate_fn.total_params\", \"blocks.6.3.se.total_ops\", \"blocks.6.3.se.total_params\", \"blocks.6.3.ghost2.total_ops\", \"blocks.6.3.ghost2.total_params\", \"blocks.6.3.ghost2.gate_fn.total_ops\", \"blocks.6.3.ghost2.gate_fn.total_params\", \"blocks.6.4.total_ops\", \"blocks.6.4.total_params\", \"blocks.6.4.ghost1.total_ops\", \"blocks.6.4.ghost1.total_params\", \"blocks.6.4.ghost1.gate_fn.total_ops\", \"blocks.6.4.ghost1.gate_fn.total_params\", \"blocks.6.4.se.total_ops\", \"blocks.6.4.se.total_params\", \"blocks.6.4.ghost2.total_ops\", \"blocks.6.4.ghost2.total_params\", \"blocks.6.4.ghost2.gate_fn.total_ops\", \"blocks.6.4.ghost2.gate_fn.total_params\", \"blocks.7.0.total_ops\", \"blocks.7.0.total_params\", \"blocks.7.0.ghost1.total_ops\", \"blocks.7.0.ghost1.total_params\", \"blocks.7.0.ghost1.gate_fn.total_ops\", \"blocks.7.0.ghost1.gate_fn.total_params\", \"blocks.7.0.se.total_ops\", \"blocks.7.0.se.total_params\", \"blocks.7.0.ghost2.total_ops\", \"blocks.7.0.ghost2.total_params\", \"blocks.7.0.ghost2.gate_fn.total_ops\", \"blocks.7.0.ghost2.gate_fn.total_params\", \"blocks.8.0.total_ops\", \"blocks.8.0.total_params\", \"blocks.8.0.ghost1.total_ops\", \"blocks.8.0.ghost1.total_params\", \"blocks.8.0.ghost1.gate_fn.total_ops\", \"blocks.8.0.ghost1.gate_fn.total_params\", \"blocks.8.0.ghost2.total_ops\", \"blocks.8.0.ghost2.total_params\", \"blocks.8.0.ghost2.gate_fn.total_ops\", \"blocks.8.0.ghost2.gate_fn.total_params\", \"blocks.8.1.total_ops\", \"blocks.8.1.total_params\", \"blocks.8.1.ghost1.total_ops\", \"blocks.8.1.ghost1.total_params\", \"blocks.8.1.ghost1.gate_fn.total_ops\", \"blocks.8.1.ghost1.gate_fn.total_params\", \"blocks.8.1.se.total_ops\", \"blocks.8.1.se.total_params\", \"blocks.8.1.ghost2.total_ops\", \"blocks.8.1.ghost2.total_params\", \"blocks.8.1.ghost2.gate_fn.total_ops\", \"blocks.8.1.ghost2.gate_fn.total_params\", \"blocks.8.2.total_ops\", \"blocks.8.2.total_params\", \"blocks.8.2.ghost1.total_ops\", \"blocks.8.2.ghost1.total_params\", \"blocks.8.2.ghost1.gate_fn.total_ops\", \"blocks.8.2.ghost1.gate_fn.total_params\", \"blocks.8.2.ghost2.total_ops\", \"blocks.8.2.ghost2.total_params\", \"blocks.8.2.ghost2.gate_fn.total_ops\", \"blocks.8.2.ghost2.gate_fn.total_params\", \"blocks.8.3.total_ops\", \"blocks.8.3.total_params\", \"blocks.8.3.ghost1.total_ops\", \"blocks.8.3.ghost1.total_params\", \"blocks.8.3.ghost1.gate_fn.total_ops\", \"blocks.8.3.ghost1.gate_fn.total_params\", \"blocks.8.3.se.total_ops\", \"blocks.8.3.se.total_params\", \"blocks.8.3.ghost2.total_ops\", \"blocks.8.3.ghost2.total_params\", \"blocks.8.3.ghost2.gate_fn.total_ops\", \"blocks.8.3.ghost2.gate_fn.total_params\", \"blocks.9.0.total_ops\", \"blocks.9.0.total_params\". "
     ]
    }
   ],
   "source": [
    "# optionally resume from a checkpoint\n",
    "resume_state, resume_epoch = resume_checkpoint(model, \"ck_ghostnetv2_10.pth.tar\")\n",
    "\n",
    "if 'optimizer' in resume_state:\n",
    "    if args.local_rank == 0:\n",
    "        logging.info('Restoring Optimizer state from checkpoint')\n",
    "    optimizer.load_state_dict(resume_state['optimizer'])\n",
    "if use_amp and 'amp' in resume_state and 'load_state_dict' in amp.__dict__:\n",
    "    if args.local_rank == 0:\n",
    "        logging.info('Restoring NVIDIA AMP state from checkpoint')\n",
    "    amp.load_state_dict(resume_state['amp'])\n",
    "del resume_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7532ef0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
