{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "210cbd4e",
   "metadata": {},
   "source": [
    "# Checking Cuda Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d9bbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from ghostnet import ghostnet\n",
    "from moganet import MogaNet\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from timm.data import create_dataset\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils import data\n",
    "from PIL import Image\n",
    "\n",
    "assert torch.cuda.is_available()\n",
    "\n",
    "# Get the GPU device name.\n",
    "device_name = torch.cuda.get_device_name()\n",
    "n_gpu = torch.cuda.device_count()\n",
    "print(f\"Found device: {device_name}, n_gpu: {n_gpu}\")\n",
    "print(f'Cuda version: {torch.version.cuda}')\n",
    "device = torch.device(\"cuda\")\n",
    "print(f'n_cpu: {os.cpu_count()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9828db",
   "metadata": {},
   "source": [
    "# Select Network and Dataset for training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c29ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "networks = ['ghostnet', 'ghostnet_prelu', 'moganet']\n",
    "datasets = ['affectnet', 'affectnet_lr', 'affectnet_lr_noise_free', 'fer2013']\n",
    "\n",
    "# Select the network and dataset.\n",
    "network = networks[2]\n",
    "dataset_name = datasets[2]\n",
    "num_mlp = 3 # (1 or 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b603c0f",
   "metadata": {},
   "source": [
    "# Loading Pre-trained Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9eb6627",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 7 if dataset_name == datasets[3] else 8\n",
    "\n",
    "def replace_relu_with_prelu(module):\n",
    "    for name, child_module in module.named_children():\n",
    "        if isinstance(child_module, nn.ReLU):\n",
    "            setattr(module, name, nn.PReLU())\n",
    "        else:\n",
    "            replace_relu_with_prelu(child_module)\n",
    "            \n",
    "def get_classifier_MLP(in_features):\n",
    "    dropout_prob = 0.5\n",
    "    num_classes = 8\n",
    "\n",
    "    if num_mlp == 1:\n",
    "        return nn.Linear(in_features, num_classes)\n",
    "    else:\n",
    "        return nn.Sequential(\n",
    "            nn.Dropout(dropout_prob).cuda(),\n",
    "            nn.Linear(in_features, 1000).cuda(),\n",
    "            nn.ReLU(inplace=True).cuda(),\n",
    "            nn.Dropout(dropout_prob).cuda(),\n",
    "            nn.Linear(1000, 256).cuda(),\n",
    "            nn.ReLU(inplace=True).cuda(),\n",
    "            nn.Linear(256, num_classes).cuda(),\n",
    "            nn.Softmax(dim=1))\n",
    "\n",
    "def load_pretrained_netwok(net = network):\n",
    "    if net == networks[0]:\n",
    "        model = ghostnet(num_classes=1000, width=1.0, dropout=0.2)\n",
    "        model.load_state_dict(torch.load('state_dict_73.98.pth'))\n",
    "        num_features = model.classifier.in_features\n",
    "        new_last_layer = get_classifier_MLP(num_features)\n",
    "        model.classifier = new_last_layer\n",
    "    elif net == networks[1]:\n",
    "        model = ghostnet(num_classes=1000, width=1.0, dropout=0.2)\n",
    "        model.load_state_dict(torch.load('state_dict_73.98.pth'))\n",
    "        num_features = model.classifier.in_features\n",
    "        new_last_layer = get_classifier_MLP(num_features)\n",
    "        model.classifier = new_last_layer\n",
    "        replace_relu_with_prelu(model)\n",
    "    elif net == networks[2]:\n",
    "        model = MogaNet()\n",
    "        model.load_state_dict(torch.load('moganet_tiny_sz224_8xbs128_ep300.pth.tar')['state_dict'])\n",
    "        num_features = model.head.in_features\n",
    "        new_last_layer = get_classifier_MLP(num_features)\n",
    "        model.head = new_last_layer\n",
    "    model = torch.nn.DataParallel(model, device_ids = list(range(1)))\n",
    "    return model\n",
    "\n",
    "model = load_pretrained_netwok()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bae0287",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5015a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AffectNet(data.Dataset):\n",
    "    def __init__(self, aff_path, mode, use_cache=True, transforms=None, force=False):\n",
    "        self.mode = mode\n",
    "        self.transforms = transforms\n",
    "        self.aff_path = aff_path\n",
    "        self.base_path = aff_path\n",
    "        \n",
    "        if mode == \"train\": \n",
    "            df = pd.read_csv('DownScaledAffectNet/new_train_labels.csv')\n",
    "        elif mode == \"test\":\n",
    "            df = pd.read_csv('DownScaledAffectNet/new_test_labels.csv')\n",
    "        \n",
    "        self.data = df \n",
    "        self.file_paths = self.data.loc[:, 'pth'].values\n",
    "        self.label = self.data.loc[:, 'label'].values\n",
    "        self.emotion_labels=['Neutral','Happiness', 'Sadness', 'Surprise', 'Fear', 'Disgust', 'Anger', 'Contempt']\n",
    "        sample_label, sample_counts = np.unique(self.label, return_counts=True)\n",
    "        self.dick = { 'neutral' :0 ,'happy' : 1, 'sad' :2 , 'surprise':3, 'fear':4, 'disgust':5, 'anger':6, 'contempt':7}\n",
    "        for l, c in zip(sample_label, sample_counts):\n",
    "            print(f'{self.emotion_labels[self.dick[l.lower()]]}: {c} ', end='')\n",
    "        print(f'\\n{len(self)} images')\n",
    "\n",
    "\n",
    "    def get_weight(self):\n",
    "        self.emotion_labels=['Neutral','Happiness', 'Sadness', 'Surprise', 'Fear', 'Disgust', 'Anger', 'Contempt']\n",
    "        self.class_to_idx = {}\n",
    "        self.idx_to_class = {}\n",
    "        for i, emotion in enumerate(self.emotion_labels):\n",
    "            self.class_to_idx[emotion] = i\n",
    "            self.idx_to_class[i] = emotion\n",
    "        sample_label, sample_counts = np.unique(self.label, return_counts=True)\n",
    "        for l, c in zip(sample_label, sample_counts):\n",
    "            print(f'{self.emotion_labels[self.dick[l.lower()]]}: {c} ', end='')\n",
    "        print('')\n",
    "        \n",
    "        cw = 1/sample_counts\n",
    "        cw /= cw.min()\n",
    "        class_weights = {self.dick[i.lower()]:cwi for i, cwi in zip(sample_label, cw)}\n",
    "        print(class_weights)\n",
    "        return class_weights\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        yo = self.file_paths[idx]\n",
    "        path = os.path.join(self.base_path, yo)\n",
    "        image = Image.open(path).convert('RGB')\n",
    "        label = self.label[idx]\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image)\n",
    "        \n",
    "        return image, self.dick[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0ba305",
   "metadata": {},
   "outputs": [],
   "source": [
    "imsize = 224\n",
    "loader = transforms.Compose([\n",
    "    transforms.Resize(imsize),  # scale imported image\n",
    "    transforms.CenterCrop(imsize),\n",
    "    transforms.ToTensor()])  # transform it into a torch tensor\n",
    "batch_size = 64\n",
    "\n",
    "def load_dataset(data_name=dataset_name):\n",
    "    if data_name == datasets[0]:\n",
    "        dataset = create_dataset(name='', root='AffectNet/affectnet/train', transform=loader)\n",
    "        test_dataset = create_dataset(name='', root='AffectNet/affectnet/val_class', transform=loader)\n",
    "        train_size = int(0.9 * len(dataset))  # 90% for training\n",
    "        val_size = len(dataset) - train_size  # Remaining 10% for test\n",
    "        train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "    elif data_name == datasets[1]:\n",
    "        dataset = create_dataset(name='', root='DownScaledAffectNet', transform=loader)\n",
    "        train_size = int(0.8 * len(dataset))  # 80% for training\n",
    "        val_size = int(0.1 * len(dataset))   # 10% for validation\n",
    "        test_size = len(dataset) - train_size - val_size  # Remaining 10% for test\n",
    "        train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "    elif data_name == datasets[2]:\n",
    "        affectnet_dir = 'DownScaledAffectNet'\n",
    "        dataset = AffectNet(affectnet_dir, 'train', transforms=loader, force=False)\n",
    "        train_size = int(0.9 * len(dataset))  # 90% for training\n",
    "        val_size = len(dataset) - train_size  # Remaining 10% for val\n",
    "        train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "        test_dataset = AffectNet(affectnet_dir, 'test', transforms=loader)\n",
    "    elif data_name == datasets[3]:\n",
    "        dataset = create_dataset(name='', root='FER-2013/train', transform=loader)\n",
    "        test_dataset = create_dataset(name='', root='FER-2013/test', transform=loader)\n",
    "        train_size = int(0.9 * len(dataset))  # 90% for training\n",
    "        val_size = len(dataset) - train_size  # Remaining 10% for val\n",
    "        train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "    num_workers = 8\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "train_loader, val_loader, test_loader = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdcbabe",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d75c02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(alpha=1, num_epochs = 50):\n",
    "    loss_fn = torch.nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "    if network == networks[2]:\n",
    "        optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=alpha*0.001, weight_decay=0.04)\n",
    "        scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[2,4,8,16,32], gamma=0.5)\n",
    "    else:\n",
    "        optimizer = torch.optim.RMSprop(filter(lambda p: p.requires_grad, model.parameters()), lr=alpha*0.0001, weight_decay=0.01, momentum=0.9)\n",
    "        scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[5,15,25], gamma=0.1)\n",
    "\n",
    "    best_acc=0\n",
    "    best_loss= float('inf')\n",
    "    best_model=None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_samples = 0\n",
    "        correct_predictions = 0\n",
    "        total_loss = 0\n",
    "\n",
    "        loop = tqdm(train_loader)\n",
    "        for images, labels in loop:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            loss = loss_fn(outputs, labels.cuda())\n",
    "\n",
    "            total_samples += labels.cuda().size(0)\n",
    "            correct_predictions += (predicted == labels.cuda()).sum().item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loop.set_description(f\"Epoch [{epoch}/{num_epochs}]\")\n",
    "            loop.set_postfix(loss=loss.item(), acc=(predicted == labels.cuda()).sum().item()/labels.cuda().size(0))\n",
    "\n",
    "        scheduler.step()\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        avg_acc = correct_predictions / total_samples\n",
    "        print(\"TRAINING\")\n",
    "        print(\"loss=\", avg_loss, \", accuracy=\", avg_acc)\n",
    "\n",
    "        total_samples = 0\n",
    "        correct_predictions = 0\n",
    "        total_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            epoch_val_accuracy = 0\n",
    "            epoch_val_loss = 0\n",
    "            for images, labels in val_loader:\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs, dim=1)\n",
    "                loss = loss_fn(outputs, labels.cuda())\n",
    "\n",
    "                total_samples += labels.cuda().size(0)\n",
    "                correct_predictions += (predicted == labels.cuda()).sum().item()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(val_loader)\n",
    "        avg_acc = correct_predictions / total_samples\n",
    "        print(\"VALIDATION\")\n",
    "        print(\"loss=\", avg_loss, \", accuracy=\", avg_acc)\n",
    "\n",
    "        if best_acc < avg_acc:\n",
    "            best_acc = avg_acc\n",
    "            best_model=copy.deepcopy(model.state_dict())\n",
    "            torch.save(model.state_dict(), './checkpoint_model_1.pth')\n",
    "        \n",
    "def FreezeParams(exception='head'):\n",
    "    for name, param in model.module.named_parameters():\n",
    "        if(exception not in name):\n",
    "            param.requires_grad = False # Freeze all layers except the last layer\n",
    "        else:\n",
    "            param.requires_grad = True\n",
    "\n",
    "if num_mlp > 1:\n",
    "    if network == networks[2]:\n",
    "        FreezeParams('head')\n",
    "    else:\n",
    "        FreezeParams('classifier')\n",
    "    train_model(num_epochs=10)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "    train_model(alpha=0.1, num_epochs=20)\n",
    "else:\n",
    "    train_model(num_epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725a9796",
   "metadata": {},
   "source": [
    "# Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab1d6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if best_model is not None:\n",
    "    model.load_state_dict(best_model)\n",
    "    print(f\"Best Validation acc:{best_acc}\")\n",
    "\n",
    "    total_samples = 0\n",
    "    correct_predictions = 0\n",
    "    total_loss = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        epoch_val_accuracy = 0\n",
    "        epoch_val_loss = 0\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            loss = loss_fn(outputs, labels.cuda())\n",
    "\n",
    "            total_samples += labels.size(0)\n",
    "            correct_predictions += (predicted == labels.cuda()).sum().item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    avg_acc = correct_predictions / total_samples\n",
    "    print(\"TESTING\")\n",
    "    print(\"loss=\", avg_loss, \", accuracy=\", avg_acc)\n",
    "    \n",
    "#     torch.save(best_model, f'ghostnet_checkpoints/best_model_0.pth')\n",
    "else:\n",
    "    print(f\"No best model Best acc:{best_acc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
